{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "super-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as tfs\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import hyperparameters\n",
    "from dataset import RobotDataset, dataset_explore\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from training import train_gan, test_gan\n",
    "from losses import generator_loss, discriminator_loss\n",
    "from metrics import ADE, FDE\n",
    "import numpy as np \n",
    "from training import trimm, reconstruct\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gentle-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_path = '/media/felpipe/Archivos HDD/SocLab/'\n",
    "path = '/home/felpipe/proyectos/Tesis/SocialPlayGround/Dataset/'\n",
    "\n",
    "seq_len = dataset_explore(path)\n",
    "\n",
    "netparams = hyperparameters(w=320, \n",
    "                            h=239, \n",
    "                            latent_dim=128, \n",
    "                            history_length=8, \n",
    "                            future_length=12,\n",
    "                            cnn_filters=[\"16\", \"32\", \"64\", \"128\", \"256\"],\n",
    "                            lin_neurons=[\"256\", \"256\"],\n",
    "                            enc_layers=2,\n",
    "                            lstm_dim=128,\n",
    "                            output_dim=8,\n",
    "                            up_criterion=0.9,\n",
    "                            down_criterion=0.0,\n",
    "                            alpha=0.15,\n",
    "                            beta=0.15,\n",
    "                            attention='add')                            \n",
    "\n",
    "data_transforms = tfs.Compose([tfs.Resize((320, 239)),\n",
    "                               tfs.ToTensor(),\n",
    "                               tfs.Normalize([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5])])\n",
    "\n",
    "data_set = RobotDataset(path, 128, seq_len, data_transforms)\n",
    "\n",
    "data_loader = DataLoader(data_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "differential-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felpipe/proyectos/Tesis/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "gen = Generator(netparams, device).to(device)\n",
    "dis = Discriminator(netparams, device).to(device)\n",
    "\n",
    "gen.eval()\n",
    "dis.eval()\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marked-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of past_routes torch.Size([8, 8, 8])\n",
      "Shape of real_routes torch.Size([8, 12, 8])\n"
     ]
    }
   ],
   "source": [
    "trimmed = trimm(batch, \n",
    "                netparams['seq_len'], \n",
    "                netparams['history'], \n",
    "                netparams['predict_seq'], \n",
    "                int(netparams['history']/2), \n",
    "                trim_mode='relative')\n",
    "\n",
    "for i in range(trimmed['steps']): \n",
    "    imgs = trimmed['imgs'][i].to(device)\n",
    "    z = trimmed['noise'][i].to(device)\n",
    "    past_routes = trimmed['past_traj'][i].to(device)\n",
    "    real_routes = trimmed['future_traj'][i].to(device)\n",
    "    past_vel = trimmed['past_vel'][i].to(device)\n",
    "    real_vel = trimmed['future_vel'][i].to(device)\n",
    "    past_obj = trimmed['past_target'][i].to(device)\n",
    "    real_obj = trimmed['future_target'][i].to(device)\n",
    "    past_routes = torch.cat((past_routes, past_vel), axis=2)\n",
    "    past_routes = torch.cat((past_routes, past_obj), axis=2)\n",
    "    real_routes = torch.cat((real_routes, real_vel), axis=2)\n",
    "    real_routes = torch.cat((real_routes, real_obj), axis=2)\n",
    "    print(f'Shape of past_routes {past_routes.shape}')\n",
    "    print(f'Shape of real_routes {real_routes.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chemical-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fake_routes torch.Size([8, 12, 8])\n",
      "Shape of real_output torch.Size([8, 1])\n",
      "Shape of fake_output torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "fake_routes = gen(imgs, z, past_routes)\n",
    "real_output = dis(imgs, real_routes, past_routes)\n",
    "fake_output = dis(imgs, fake_routes, past_routes)\n",
    "\n",
    "print(f'Shape of fake_routes {fake_routes.shape}')\n",
    "print(f'Shape of real_output {real_output.shape}')\n",
    "print(f'Shape of fake_output {fake_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5812401\n",
      "1.0492758\n"
     ]
    }
   ],
   "source": [
    "print(ADE(real_routes, fake_routes))\n",
    "print(FDE(real_routes, fake_routes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "furnished-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_loss = discriminator_loss(real_output, fake_output, netparams)\n",
    "gen_loss = generator_loss(fake_output, fake_routes, real_routes, netparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
